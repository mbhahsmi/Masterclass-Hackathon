{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lib = 'train'\n",
    "train_dir = '/root/data/train/'\n",
    "\n",
    "val_lib = ''\n",
    "val_dir = ''\n",
    "\n",
    "project='M1'             # Which MUTATION, choose from M1/M2/M3/M4 ...\n",
    "output = os.path.join('Output', project)\n",
    "\n",
    "batch_size = 128\n",
    "nepochs = 20\n",
    "\n",
    "test_every = 1           # related to validation set, if needed\n",
    "weights = 0.5            # weight of a positive class if imbalanced\n",
    "\n",
    "lr = 1e-4                # learning rate\n",
    "weight_decay = 1e-4      # l2 regularzation weight\n",
    " \n",
    "best_auc_v = 0           # related to validation set, if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "class MPdataset(data.Dataset):\n",
    "    def __init__(self, libraryfile='', path_dir=None, project=None, transform=None, mult=2):         \n",
    "            \n",
    "        lib = pd.DataFrame(pd.read_csv(libraryfile, usecols = ['SLIDES', project], keep_default_na=True))\n",
    "        lib.dropna(inplace=True)\n",
    "        \n",
    "        tar = lib[project].values.tolist()\n",
    "        allslides = lib['SLIDES'].values.tolist()       \n",
    "        slides = []\n",
    "        tiles = []\n",
    "        ntiles = []\n",
    "        slideIDX = []\n",
    "        targets = []\n",
    "        j = 0\n",
    "        for i, path in enumerate(allslides):\n",
    "            t = []\n",
    "            cpath = os.path.join(path_dir, str(path))\n",
    "            for f in os.listdir(cpath): \n",
    "                if '.jpg' in f:\n",
    "                    t.append(os.path.join(cpath, f))\n",
    "            if len(t) > 0:\n",
    "                slides.append(path)\n",
    "                tiles.extend(t)\n",
    "                ntiles.append(len(t))\n",
    "                slideIDX.extend([j]*len(t))\n",
    "                targets.append(int(tar[i]))\n",
    "                j+=1\n",
    "                \n",
    "        print('Number of Slides: {}'.format(len(slides)))\n",
    "        print('Number of tiles: {}'.format(len(tiles)))\n",
    "        self.slides = slides\n",
    "        self.slideIDX = slideIDX\n",
    "        self.ntiles = ntiles\n",
    "        self.tiles = tiles\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        self.mult = mult\n",
    "        self.mode = None\n",
    "\n",
    "    def setmode(self,mode):\n",
    "        self.mode = mode\n",
    "    def maketraindata(self, idxs):\n",
    "        self.t_data = [(self.slideIDX[x],self.tiles[x],self.targets[self.slideIDX[x]]) for x in idxs]\n",
    "    def shuffletraindata(self):\n",
    "        self.t_data = random.sample(self.t_data, len(self.t_data))\n",
    "    def __getitem__(self,index):\n",
    "        if self.mode == 1:                                     # loads all tiles from each slide sequentially for train/validatoin set\n",
    "            tile = self.tiles[index]\n",
    "            img = Image.open(str(tile)).convert('RGB')\n",
    "            slideIDX = self.slideIDX[index]\n",
    "            target = self.targets[slideIDX]\n",
    "            if self.mult != 1:\n",
    "                img = img.resize((224,224),Image.BILINEAR)\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img, target                       \n",
    "        elif self.mode == 2:                                  # used when a different trainset is prepared e.g. with given tile index                     \n",
    "            slideIDX, tile, target = self.t_data[index]\n",
    "            img = Image.open(str(tile)).convert('RGB')\n",
    "            if self.mult != 1:\n",
    "                img = img.resize((224,224),Image.BILINEAR)\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img, target\n",
    "    def __len__(self):\n",
    "        if self.mode == 1:\n",
    "            return len(self.tiles)\n",
    "        elif self.mode == 2:\n",
    "            return len(self.t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_roc_auc(target, prediction):\n",
    "    fpr, tpr, thresholds = roc_curve(target, prediction)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return roc_auc\n",
    "\n",
    "def calculate_accuracy(output, target):\n",
    "    preds = output.max(1, keepdim=True)[1]\n",
    "    correct = preds.eq(target.view_as(preds)).sum()\n",
    "    acc = correct.float()/preds.shape[0]\n",
    "    return acc\n",
    "\n",
    "#function to calculate mean of data grouped per slide, used for aggregating tile scores into slide score\n",
    "def group_avg(groups, data):\n",
    "    order = np.lexsort((data, groups))\n",
    "    groups = groups[order]\n",
    "    data = data[order]\n",
    "    unames, idx, counts = np.unique(groups, return_inverse=True, return_counts=True)\n",
    "    group_sum = np.bincount(idx, weights=data)\n",
    "    group_average = group_sum / counts\n",
    "    return group_average\n",
    "\n",
    "#function to find index of max value in data grouped per slide\n",
    "def group_max(groups, data, nmax):\n",
    "    out = np.empty(nmax)\n",
    "    out[:] = np.nan\n",
    "    order = np.lexsort((data, groups))\n",
    "    groups = groups[order]\n",
    "    data = data[order]\n",
    "    index = np.empty(len(groups), 'bool')\n",
    "    index[-1] = True\n",
    "    index[:-1] = groups[1:] != groups[:-1]\n",
    "    out[groups[index]] = data[index]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline cnn model to fine tune\n",
    "model = models.resnet18(True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model.cuda()\n",
    "\n",
    "if weights==0.5:\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "else:\n",
    "    w = torch.Tensor([1-weights,weights])\n",
    "    criterion = nn.CrossEntropyLoss(w).cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=lr)\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Slides: 238\n",
      "Number of tiles: 239904\n"
     ]
    }
   ],
   "source": [
    "# normalization\n",
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                 std=[0.1, 0.1, 0.1])\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize, ])\n",
    "\n",
    "trans_Valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "#loading data using cutom dataloader class\n",
    "train_dset = MPdataset(train_lib, train_dir, project, trans)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dset,\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=6, pin_memory=False)\n",
    "\n",
    "if val_lib:\n",
    "    val_dset = MPdataset(val_lib, val_dir, project, trans_Valid)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dset,\n",
    "        batch_size=batch_size, shuffle=False,\n",
    "        num_workers=6, pin_memory=False)\n",
    "\n",
    "#open output file, \n",
    "fconv = open(os.path.join(output,'train_convergence.csv'), 'w')\n",
    "fconv.write('epoch,loss,accuracy\\n')\n",
    "fconv.close()\n",
    "fconv = open(os.path.join(output, 'valid_convergence.csv'), 'w')\n",
    "fconv.write('epoch,tile-acc,max-auc,auc-avg-prob,auc-mjvt,auc-best\\n')\n",
    "fconv.close()\n",
    "\n",
    "num_tiles = len(train_dset.slideIDX)\n",
    "\n",
    "# making trainset of all tiles of training set slides\n",
    "train_dset.maketraindata(np.arange(num_tiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(run, loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    for i, (input, target) in enumerate(loader):\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()*input.size(0)\n",
    "        acc = calculate_accuracy(output, target)\n",
    "        running_acc += acc.item()*input.size(0)\n",
    "        if i%100 == 0:\n",
    "            print(\"Train Epoch: [{:3d}/{:3d}] Batch number: {:3d}, Training: Loss: {:.4f}, Accuracy: {:.2f}%\".\n",
    "              format(run+1, nepochs, i+1, running_loss/((i+1)*input.size(0)), (100*running_acc)/((i+1)* input.size(0))))\n",
    "\n",
    "    return running_loss/len(loader.dataset), running_acc/len(loader.dataset)\n",
    "\n",
    "def inference(run, loader, model):\n",
    "    model.eval()\n",
    "    running_acc = 0.\n",
    "    probs = torch.FloatTensor(len(loader.dataset))\n",
    "    preds = torch.FloatTensor(len(loader.dataset))\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(loader):\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "            output = model(input)\n",
    "            acc = calculate_accuracy(output, target)\n",
    "            y = F.softmax(output, dim=1)\n",
    "            _, pr = torch.max(output.data, 1)\n",
    "            preds[i * batch_size:i * batch_size + input.size(0)] = pr.detach().clone()\n",
    "            probs[i * batch_size:i * batch_size + input.size(0)] = y.detach()[:, 1].clone()\n",
    "            running_acc += acc.item() * input.size(0)\n",
    "            if i % 100 == 0:\n",
    "                print('Inference\\tEpoch: [{:3d}/{:3d}]\\tBatch: [{:3d}/{}]\\t acc: {:0.2f}%'\n",
    "                  .format(run + 1, nepochs, i + 1, len(loader), (100*running_acc)/((i+1)*input.size(0))))\n",
    "    return probs.cpu().numpy(), running_acc/len(loader.dataset), preds.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: [  1/ 20] Batch number:   1, Training: Loss: 0.9301, Accuracy: 29.69%\n",
      "Train Epoch: [  1/ 20] Batch number: 101, Training: Loss: 0.3879, Accuracy: 81.92%\n",
      "Train Epoch: [  1/ 20] Batch number: 201, Training: Loss: 0.3216, Accuracy: 85.78%\n",
      "Train Epoch: [  1/ 20] Batch number: 301, Training: Loss: 0.2846, Accuracy: 87.65%\n",
      "Train Epoch: [  1/ 20] Batch number: 401, Training: Loss: 0.2594, Accuracy: 88.87%\n",
      "Train Epoch: [  1/ 20] Batch number: 501, Training: Loss: 0.2402, Accuracy: 89.82%\n",
      "Train Epoch: [  1/ 20] Batch number: 601, Training: Loss: 0.2242, Accuracy: 90.58%\n",
      "Train Epoch: [  1/ 20] Batch number: 701, Training: Loss: 0.2115, Accuracy: 91.16%\n",
      "Train Epoch: [  1/ 20] Batch number: 801, Training: Loss: 0.2002, Accuracy: 91.67%\n",
      "Train Epoch: [  1/ 20] Batch number: 901, Training: Loss: 0.1912, Accuracy: 92.08%\n",
      "Train Epoch: [  1/ 20] Batch number: 1001, Training: Loss: 0.1828, Accuracy: 92.43%\n",
      "Train Epoch: [  1/ 20] Batch number: 1101, Training: Loss: 0.1758, Accuracy: 92.76%\n",
      "Train Epoch: [  1/ 20] Batch number: 1201, Training: Loss: 0.1694, Accuracy: 93.05%\n",
      "Train Epoch: [  1/ 20] Batch number: 1301, Training: Loss: 0.1635, Accuracy: 93.29%\n",
      "Train Epoch: [  1/ 20] Batch number: 1401, Training: Loss: 0.1581, Accuracy: 93.53%\n",
      "Train Epoch: [  1/ 20] Batch number: 1501, Training: Loss: 0.1532, Accuracy: 93.74%\n",
      "Train Epoch: [  1/ 20] Batch number: 1601, Training: Loss: 0.1493, Accuracy: 93.91%\n",
      "Train Epoch: [  1/ 20] Batch number: 1701, Training: Loss: 0.1454, Accuracy: 94.08%\n",
      "Train Epoch: [  1/ 20] Batch number: 1801, Training: Loss: 0.1416, Accuracy: 94.24%\n",
      "--- 4.29 minutes ---\n",
      "Training\tEpoch: [1/20]\tLoss: 0.1391\tAccuracy: 0.9435\n",
      "--- 4.29 minutes ---\n",
      "Train Epoch: [  2/ 20] Batch number:   1, Training: Loss: 0.0487, Accuracy: 98.44%\n",
      "Train Epoch: [  2/ 20] Batch number: 101, Training: Loss: 0.0539, Accuracy: 97.87%\n",
      "Train Epoch: [  2/ 20] Batch number: 201, Training: Loss: 0.0528, Accuracy: 97.92%\n",
      "Train Epoch: [  2/ 20] Batch number: 301, Training: Loss: 0.0547, Accuracy: 97.89%\n",
      "Train Epoch: [  2/ 20] Batch number: 401, Training: Loss: 0.0531, Accuracy: 97.96%\n",
      "Train Epoch: [  2/ 20] Batch number: 501, Training: Loss: 0.0532, Accuracy: 97.95%\n",
      "Train Epoch: [  2/ 20] Batch number: 601, Training: Loss: 0.0534, Accuracy: 97.95%\n",
      "Train Epoch: [  2/ 20] Batch number: 701, Training: Loss: 0.0539, Accuracy: 97.93%\n",
      "Train Epoch: [  2/ 20] Batch number: 801, Training: Loss: 0.0531, Accuracy: 97.95%\n",
      "Train Epoch: [  2/ 20] Batch number: 901, Training: Loss: 0.0525, Accuracy: 97.98%\n",
      "Train Epoch: [  2/ 20] Batch number: 1001, Training: Loss: 0.0527, Accuracy: 97.97%\n",
      "Train Epoch: [  2/ 20] Batch number: 1101, Training: Loss: 0.0526, Accuracy: 97.99%\n",
      "Train Epoch: [  2/ 20] Batch number: 1201, Training: Loss: 0.0524, Accuracy: 97.99%\n",
      "Train Epoch: [  2/ 20] Batch number: 1301, Training: Loss: 0.0520, Accuracy: 98.02%\n",
      "Train Epoch: [  2/ 20] Batch number: 1401, Training: Loss: 0.0515, Accuracy: 98.02%\n",
      "Train Epoch: [  2/ 20] Batch number: 1501, Training: Loss: 0.0515, Accuracy: 98.03%\n",
      "Train Epoch: [  2/ 20] Batch number: 1601, Training: Loss: 0.0520, Accuracy: 98.02%\n",
      "Train Epoch: [  2/ 20] Batch number: 1701, Training: Loss: 0.0519, Accuracy: 98.02%\n",
      "Train Epoch: [  2/ 20] Batch number: 1801, Training: Loss: 0.0516, Accuracy: 98.03%\n",
      "--- 8.53 minutes ---\n",
      "Training\tEpoch: [2/20]\tLoss: 0.0515\tAccuracy: 0.9804\n",
      "--- 8.53 minutes ---\n",
      "Train Epoch: [  3/ 20] Batch number:   1, Training: Loss: 0.0258, Accuracy: 99.22%\n",
      "Train Epoch: [  3/ 20] Batch number: 101, Training: Loss: 0.0362, Accuracy: 98.62%\n",
      "Train Epoch: [  3/ 20] Batch number: 201, Training: Loss: 0.0340, Accuracy: 98.68%\n",
      "Train Epoch: [  3/ 20] Batch number: 301, Training: Loss: 0.0336, Accuracy: 98.72%\n",
      "Train Epoch: [  3/ 20] Batch number: 401, Training: Loss: 0.0322, Accuracy: 98.77%\n",
      "Train Epoch: [  3/ 20] Batch number: 501, Training: Loss: 0.0320, Accuracy: 98.77%\n",
      "Train Epoch: [  3/ 20] Batch number: 601, Training: Loss: 0.0322, Accuracy: 98.77%\n",
      "Train Epoch: [  3/ 20] Batch number: 701, Training: Loss: 0.0323, Accuracy: 98.75%\n",
      "Train Epoch: [  3/ 20] Batch number: 801, Training: Loss: 0.0335, Accuracy: 98.71%\n",
      "Train Epoch: [  3/ 20] Batch number: 901, Training: Loss: 0.0342, Accuracy: 98.69%\n",
      "Train Epoch: [  3/ 20] Batch number: 1001, Training: Loss: 0.0341, Accuracy: 98.70%\n",
      "Train Epoch: [  3/ 20] Batch number: 1101, Training: Loss: 0.0343, Accuracy: 98.70%\n",
      "Train Epoch: [  3/ 20] Batch number: 1201, Training: Loss: 0.0344, Accuracy: 98.70%\n",
      "Train Epoch: [  3/ 20] Batch number: 1301, Training: Loss: 0.0343, Accuracy: 98.70%\n",
      "Train Epoch: [  3/ 20] Batch number: 1401, Training: Loss: 0.0345, Accuracy: 98.70%\n",
      "Train Epoch: [  3/ 20] Batch number: 1501, Training: Loss: 0.0349, Accuracy: 98.69%\n",
      "Train Epoch: [  3/ 20] Batch number: 1601, Training: Loss: 0.0352, Accuracy: 98.67%\n",
      "Train Epoch: [  3/ 20] Batch number: 1701, Training: Loss: 0.0354, Accuracy: 98.67%\n",
      "Train Epoch: [  3/ 20] Batch number: 1801, Training: Loss: 0.0352, Accuracy: 98.68%\n",
      "--- 12.67 minutes ---\n",
      "Training\tEpoch: [3/20]\tLoss: 0.0351\tAccuracy: 0.9869\n",
      "--- 12.67 minutes ---\n",
      "Train Epoch: [  4/ 20] Batch number:   1, Training: Loss: 0.0163, Accuracy: 99.22%\n",
      "Train Epoch: [  4/ 20] Batch number: 101, Training: Loss: 0.0245, Accuracy: 99.13%\n",
      "Train Epoch: [  4/ 20] Batch number: 201, Training: Loss: 0.0243, Accuracy: 99.10%\n",
      "Train Epoch: [  4/ 20] Batch number: 301, Training: Loss: 0.0249, Accuracy: 99.05%\n",
      "Train Epoch: [  4/ 20] Batch number: 401, Training: Loss: 0.0239, Accuracy: 99.10%\n",
      "Train Epoch: [  4/ 20] Batch number: 501, Training: Loss: 0.0238, Accuracy: 99.12%\n",
      "Train Epoch: [  4/ 20] Batch number: 601, Training: Loss: 0.0237, Accuracy: 99.13%\n",
      "Train Epoch: [  4/ 20] Batch number: 701, Training: Loss: 0.0241, Accuracy: 99.10%\n",
      "Train Epoch: [  4/ 20] Batch number: 801, Training: Loss: 0.0245, Accuracy: 99.08%\n",
      "Train Epoch: [  4/ 20] Batch number: 901, Training: Loss: 0.0257, Accuracy: 99.04%\n",
      "Train Epoch: [  4/ 20] Batch number: 1001, Training: Loss: 0.0261, Accuracy: 99.03%\n",
      "Train Epoch: [  4/ 20] Batch number: 1101, Training: Loss: 0.0261, Accuracy: 99.03%\n",
      "Train Epoch: [  4/ 20] Batch number: 1201, Training: Loss: 0.0267, Accuracy: 99.01%\n",
      "Train Epoch: [  4/ 20] Batch number: 1301, Training: Loss: 0.0266, Accuracy: 99.02%\n",
      "Train Epoch: [  4/ 20] Batch number: 1401, Training: Loss: 0.0264, Accuracy: 99.03%\n",
      "Train Epoch: [  4/ 20] Batch number: 1501, Training: Loss: 0.0263, Accuracy: 99.04%\n",
      "Train Epoch: [  4/ 20] Batch number: 1601, Training: Loss: 0.0269, Accuracy: 99.01%\n",
      "Train Epoch: [  4/ 20] Batch number: 1701, Training: Loss: 0.0270, Accuracy: 99.00%\n",
      "Train Epoch: [  4/ 20] Batch number: 1801, Training: Loss: 0.0270, Accuracy: 99.00%\n",
      "--- 16.80 minutes ---\n",
      "Training\tEpoch: [4/20]\tLoss: 0.0269\tAccuracy: 0.9900\n",
      "--- 16.80 minutes ---\n",
      "Train Epoch: [  5/ 20] Batch number:   1, Training: Loss: 0.0400, Accuracy: 97.66%\n",
      "Train Epoch: [  5/ 20] Batch number: 101, Training: Loss: 0.0260, Accuracy: 98.96%\n",
      "Train Epoch: [  5/ 20] Batch number: 201, Training: Loss: 0.0274, Accuracy: 98.97%\n",
      "Train Epoch: [  5/ 20] Batch number: 301, Training: Loss: 0.0253, Accuracy: 99.07%\n",
      "Train Epoch: [  5/ 20] Batch number: 401, Training: Loss: 0.0240, Accuracy: 99.14%\n",
      "Train Epoch: [  5/ 20] Batch number: 501, Training: Loss: 0.0241, Accuracy: 99.13%\n",
      "Train Epoch: [  5/ 20] Batch number: 601, Training: Loss: 0.0242, Accuracy: 99.12%\n",
      "Train Epoch: [  5/ 20] Batch number: 701, Training: Loss: 0.0242, Accuracy: 99.11%\n",
      "Train Epoch: [  5/ 20] Batch number: 801, Training: Loss: 0.0246, Accuracy: 99.09%\n",
      "Train Epoch: [  5/ 20] Batch number: 901, Training: Loss: 0.0247, Accuracy: 99.09%\n",
      "Train Epoch: [  5/ 20] Batch number: 1001, Training: Loss: 0.0243, Accuracy: 99.10%\n",
      "Train Epoch: [  5/ 20] Batch number: 1101, Training: Loss: 0.0237, Accuracy: 99.12%\n",
      "Train Epoch: [  5/ 20] Batch number: 1201, Training: Loss: 0.0235, Accuracy: 99.13%\n",
      "Train Epoch: [  5/ 20] Batch number: 1301, Training: Loss: 0.0237, Accuracy: 99.12%\n",
      "Train Epoch: [  5/ 20] Batch number: 1401, Training: Loss: 0.0237, Accuracy: 99.12%\n",
      "Train Epoch: [  5/ 20] Batch number: 1501, Training: Loss: 0.0242, Accuracy: 99.10%\n",
      "Train Epoch: [  5/ 20] Batch number: 1601, Training: Loss: 0.0241, Accuracy: 99.11%\n",
      "Train Epoch: [  5/ 20] Batch number: 1701, Training: Loss: 0.0240, Accuracy: 99.11%\n",
      "Train Epoch: [  5/ 20] Batch number: 1801, Training: Loss: 0.0243, Accuracy: 99.11%\n",
      "--- 21.10 minutes ---\n",
      "Training\tEpoch: [5/20]\tLoss: 0.0243\tAccuracy: 0.9911\n",
      "--- 21.10 minutes ---\n",
      "Train Epoch: [  6/ 20] Batch number:   1, Training: Loss: 0.0089, Accuracy: 99.22%\n",
      "Train Epoch: [  6/ 20] Batch number: 101, Training: Loss: 0.0129, Accuracy: 99.55%\n",
      "Train Epoch: [  6/ 20] Batch number: 201, Training: Loss: 0.0132, Accuracy: 99.53%\n",
      "Train Epoch: [  6/ 20] Batch number: 301, Training: Loss: 0.0133, Accuracy: 99.52%\n",
      "Train Epoch: [  6/ 20] Batch number: 401, Training: Loss: 0.0133, Accuracy: 99.53%\n",
      "Train Epoch: [  6/ 20] Batch number: 501, Training: Loss: 0.0147, Accuracy: 99.48%\n",
      "Train Epoch: [  6/ 20] Batch number: 601, Training: Loss: 0.0164, Accuracy: 99.41%\n",
      "Train Epoch: [  6/ 20] Batch number: 701, Training: Loss: 0.0179, Accuracy: 99.35%\n",
      "Train Epoch: [  6/ 20] Batch number: 801, Training: Loss: 0.0186, Accuracy: 99.33%\n",
      "Train Epoch: [  6/ 20] Batch number: 901, Training: Loss: 0.0189, Accuracy: 99.32%\n",
      "Train Epoch: [  6/ 20] Batch number: 1001, Training: Loss: 0.0189, Accuracy: 99.33%\n",
      "Train Epoch: [  6/ 20] Batch number: 1101, Training: Loss: 0.0188, Accuracy: 99.33%\n",
      "Train Epoch: [  6/ 20] Batch number: 1201, Training: Loss: 0.0192, Accuracy: 99.32%\n",
      "Train Epoch: [  6/ 20] Batch number: 1301, Training: Loss: 0.0191, Accuracy: 99.31%\n",
      "Train Epoch: [  6/ 20] Batch number: 1401, Training: Loss: 0.0192, Accuracy: 99.31%\n",
      "Train Epoch: [  6/ 20] Batch number: 1501, Training: Loss: 0.0192, Accuracy: 99.31%\n",
      "Train Epoch: [  6/ 20] Batch number: 1601, Training: Loss: 0.0195, Accuracy: 99.30%\n",
      "Train Epoch: [  6/ 20] Batch number: 1701, Training: Loss: 0.0194, Accuracy: 99.31%\n",
      "Train Epoch: [  6/ 20] Batch number: 1801, Training: Loss: 0.0197, Accuracy: 99.30%\n",
      "--- 25.25 minutes ---\n",
      "Training\tEpoch: [6/20]\tLoss: 0.0197\tAccuracy: 0.9930\n",
      "--- 25.25 minutes ---\n",
      "Train Epoch: [  7/ 20] Batch number:   1, Training: Loss: 0.0535, Accuracy: 98.44%\n",
      "Train Epoch: [  7/ 20] Batch number: 101, Training: Loss: 0.0179, Accuracy: 99.28%\n",
      "Train Epoch: [  7/ 20] Batch number: 201, Training: Loss: 0.0157, Accuracy: 99.39%\n",
      "Train Epoch: [  7/ 20] Batch number: 301, Training: Loss: 0.0141, Accuracy: 99.48%\n",
      "Train Epoch: [  7/ 20] Batch number: 401, Training: Loss: 0.0137, Accuracy: 99.48%\n",
      "Train Epoch: [  7/ 20] Batch number: 501, Training: Loss: 0.0139, Accuracy: 99.48%\n",
      "Train Epoch: [  7/ 20] Batch number: 601, Training: Loss: 0.0149, Accuracy: 99.46%\n",
      "Train Epoch: [  7/ 20] Batch number: 701, Training: Loss: 0.0154, Accuracy: 99.43%\n",
      "Train Epoch: [  7/ 20] Batch number: 801, Training: Loss: 0.0167, Accuracy: 99.39%\n",
      "Train Epoch: [  7/ 20] Batch number: 901, Training: Loss: 0.0167, Accuracy: 99.39%\n",
      "Train Epoch: [  7/ 20] Batch number: 1001, Training: Loss: 0.0172, Accuracy: 99.38%\n",
      "Train Epoch: [  7/ 20] Batch number: 1101, Training: Loss: 0.0174, Accuracy: 99.37%\n",
      "Train Epoch: [  7/ 20] Batch number: 1201, Training: Loss: 0.0171, Accuracy: 99.39%\n",
      "Train Epoch: [  7/ 20] Batch number: 1301, Training: Loss: 0.0170, Accuracy: 99.39%\n",
      "Train Epoch: [  7/ 20] Batch number: 1401, Training: Loss: 0.0170, Accuracy: 99.38%\n",
      "Train Epoch: [  7/ 20] Batch number: 1501, Training: Loss: 0.0172, Accuracy: 99.37%\n",
      "Train Epoch: [  7/ 20] Batch number: 1601, Training: Loss: 0.0175, Accuracy: 99.36%\n",
      "Train Epoch: [  7/ 20] Batch number: 1701, Training: Loss: 0.0182, Accuracy: 99.33%\n",
      "Train Epoch: [  7/ 20] Batch number: 1801, Training: Loss: 0.0182, Accuracy: 99.33%\n",
      "--- 29.39 minutes ---\n",
      "Training\tEpoch: [7/20]\tLoss: 0.0183\tAccuracy: 0.9933\n",
      "--- 29.39 minutes ---\n",
      "Train Epoch: [  8/ 20] Batch number:   1, Training: Loss: 0.0021, Accuracy: 100.00%\n",
      "Train Epoch: [  8/ 20] Batch number: 101, Training: Loss: 0.0143, Accuracy: 99.53%\n",
      "Train Epoch: [  8/ 20] Batch number: 201, Training: Loss: 0.0137, Accuracy: 99.51%\n",
      "Train Epoch: [  8/ 20] Batch number: 301, Training: Loss: 0.0131, Accuracy: 99.55%\n",
      "Train Epoch: [  8/ 20] Batch number: 401, Training: Loss: 0.0130, Accuracy: 99.53%\n",
      "Train Epoch: [  8/ 20] Batch number: 501, Training: Loss: 0.0129, Accuracy: 99.53%\n",
      "Train Epoch: [  8/ 20] Batch number: 601, Training: Loss: 0.0134, Accuracy: 99.52%\n",
      "Train Epoch: [  8/ 20] Batch number: 701, Training: Loss: 0.0146, Accuracy: 99.49%\n",
      "Train Epoch: [  8/ 20] Batch number: 801, Training: Loss: 0.0152, Accuracy: 99.47%\n",
      "Train Epoch: [  8/ 20] Batch number: 901, Training: Loss: 0.0154, Accuracy: 99.46%\n",
      "Train Epoch: [  8/ 20] Batch number: 1001, Training: Loss: 0.0157, Accuracy: 99.45%\n",
      "Train Epoch: [  8/ 20] Batch number: 1101, Training: Loss: 0.0155, Accuracy: 99.45%\n",
      "Train Epoch: [  8/ 20] Batch number: 1201, Training: Loss: 0.0165, Accuracy: 99.41%\n",
      "Train Epoch: [  8/ 20] Batch number: 1301, Training: Loss: 0.0166, Accuracy: 99.41%\n",
      "Train Epoch: [  8/ 20] Batch number: 1401, Training: Loss: 0.0163, Accuracy: 99.42%\n",
      "Train Epoch: [  8/ 20] Batch number: 1501, Training: Loss: 0.0162, Accuracy: 99.42%\n",
      "Train Epoch: [  8/ 20] Batch number: 1601, Training: Loss: 0.0164, Accuracy: 99.42%\n",
      "Train Epoch: [  8/ 20] Batch number: 1701, Training: Loss: 0.0168, Accuracy: 99.41%\n",
      "Train Epoch: [  8/ 20] Batch number: 1801, Training: Loss: 0.0167, Accuracy: 99.41%\n",
      "--- 33.53 minutes ---\n",
      "Training\tEpoch: [8/20]\tLoss: 0.0166\tAccuracy: 0.9941\n",
      "--- 33.53 minutes ---\n",
      "Train Epoch: [  9/ 20] Batch number:   1, Training: Loss: 0.0169, Accuracy: 99.22%\n",
      "Train Epoch: [  9/ 20] Batch number: 101, Training: Loss: 0.0132, Accuracy: 99.50%\n",
      "Train Epoch: [  9/ 20] Batch number: 201, Training: Loss: 0.0126, Accuracy: 99.54%\n",
      "Train Epoch: [  9/ 20] Batch number: 301, Training: Loss: 0.0114, Accuracy: 99.59%\n",
      "Train Epoch: [  9/ 20] Batch number: 401, Training: Loss: 0.0127, Accuracy: 99.55%\n",
      "Train Epoch: [  9/ 20] Batch number: 501, Training: Loss: 0.0135, Accuracy: 99.51%\n",
      "Train Epoch: [  9/ 20] Batch number: 601, Training: Loss: 0.0141, Accuracy: 99.49%\n",
      "Train Epoch: [  9/ 20] Batch number: 701, Training: Loss: 0.0147, Accuracy: 99.47%\n",
      "Train Epoch: [  9/ 20] Batch number: 801, Training: Loss: 0.0146, Accuracy: 99.47%\n",
      "Train Epoch: [  9/ 20] Batch number: 901, Training: Loss: 0.0147, Accuracy: 99.47%\n",
      "Train Epoch: [  9/ 20] Batch number: 1001, Training: Loss: 0.0145, Accuracy: 99.48%\n",
      "Train Epoch: [  9/ 20] Batch number: 1101, Training: Loss: 0.0143, Accuracy: 99.49%\n",
      "Train Epoch: [  9/ 20] Batch number: 1201, Training: Loss: 0.0141, Accuracy: 99.50%\n",
      "Train Epoch: [  9/ 20] Batch number: 1301, Training: Loss: 0.0142, Accuracy: 99.50%\n",
      "Train Epoch: [  9/ 20] Batch number: 1401, Training: Loss: 0.0144, Accuracy: 99.49%\n",
      "Train Epoch: [  9/ 20] Batch number: 1501, Training: Loss: 0.0146, Accuracy: 99.48%\n",
      "Train Epoch: [  9/ 20] Batch number: 1601, Training: Loss: 0.0147, Accuracy: 99.48%\n",
      "Train Epoch: [  9/ 20] Batch number: 1701, Training: Loss: 0.0146, Accuracy: 99.48%\n",
      "Train Epoch: [  9/ 20] Batch number: 1801, Training: Loss: 0.0148, Accuracy: 99.47%\n",
      "--- 37.66 minutes ---\n",
      "Training\tEpoch: [9/20]\tLoss: 0.0149\tAccuracy: 0.9946\n",
      "--- 37.66 minutes ---\n",
      "Train Epoch: [ 10/ 20] Batch number:   1, Training: Loss: 0.0065, Accuracy: 100.00%\n",
      "Train Epoch: [ 10/ 20] Batch number: 101, Training: Loss: 0.0204, Accuracy: 99.30%\n",
      "Train Epoch: [ 10/ 20] Batch number: 201, Training: Loss: 0.0183, Accuracy: 99.38%\n",
      "Train Epoch: [ 10/ 20] Batch number: 301, Training: Loss: 0.0168, Accuracy: 99.42%\n",
      "Train Epoch: [ 10/ 20] Batch number: 401, Training: Loss: 0.0151, Accuracy: 99.50%\n",
      "Train Epoch: [ 10/ 20] Batch number: 501, Training: Loss: 0.0149, Accuracy: 99.50%\n",
      "Train Epoch: [ 10/ 20] Batch number: 601, Training: Loss: 0.0146, Accuracy: 99.51%\n",
      "Train Epoch: [ 10/ 20] Batch number: 701, Training: Loss: 0.0141, Accuracy: 99.53%\n",
      "Train Epoch: [ 10/ 20] Batch number: 801, Training: Loss: 0.0138, Accuracy: 99.54%\n",
      "Train Epoch: [ 10/ 20] Batch number: 901, Training: Loss: 0.0134, Accuracy: 99.55%\n",
      "Train Epoch: [ 10/ 20] Batch number: 1001, Training: Loss: 0.0138, Accuracy: 99.53%\n",
      "Train Epoch: [ 10/ 20] Batch number: 1101, Training: Loss: 0.0136, Accuracy: 99.53%\n",
      "Train Epoch: [ 10/ 20] Batch number: 1201, Training: Loss: 0.0139, Accuracy: 99.52%\n",
      "Train Epoch: [ 10/ 20] Batch number: 1301, Training: Loss: 0.0141, Accuracy: 99.51%\n",
      "Train Epoch: [ 10/ 20] Batch number: 1401, Training: Loss: 0.0141, Accuracy: 99.50%\n",
      "Train Epoch: [ 10/ 20] Batch number: 1501, Training: Loss: 0.0139, Accuracy: 99.51%\n",
      "Train Epoch: [ 10/ 20] Batch number: 1601, Training: Loss: 0.0138, Accuracy: 99.51%\n",
      "Train Epoch: [ 10/ 20] Batch number: 1701, Training: Loss: 0.0136, Accuracy: 99.52%\n",
      "Train Epoch: [ 10/ 20] Batch number: 1801, Training: Loss: 0.0137, Accuracy: 99.51%\n",
      "--- 41.85 minutes ---\n",
      "Training\tEpoch: [10/20]\tLoss: 0.0141\tAccuracy: 0.9950\n",
      "--- 41.85 minutes ---\n",
      "Train Epoch: [ 11/ 20] Batch number:   1, Training: Loss: 0.0783, Accuracy: 98.44%\n",
      "Train Epoch: [ 11/ 20] Batch number: 101, Training: Loss: 0.0109, Accuracy: 99.60%\n",
      "Train Epoch: [ 11/ 20] Batch number: 201, Training: Loss: 0.0115, Accuracy: 99.61%\n",
      "Train Epoch: [ 11/ 20] Batch number: 301, Training: Loss: 0.0121, Accuracy: 99.58%\n",
      "Train Epoch: [ 11/ 20] Batch number: 401, Training: Loss: 0.0130, Accuracy: 99.56%\n",
      "Train Epoch: [ 11/ 20] Batch number: 501, Training: Loss: 0.0129, Accuracy: 99.56%\n",
      "Train Epoch: [ 11/ 20] Batch number: 601, Training: Loss: 0.0131, Accuracy: 99.54%\n",
      "Train Epoch: [ 11/ 20] Batch number: 701, Training: Loss: 0.0132, Accuracy: 99.54%\n",
      "Train Epoch: [ 11/ 20] Batch number: 801, Training: Loss: 0.0129, Accuracy: 99.55%\n",
      "Train Epoch: [ 11/ 20] Batch number: 901, Training: Loss: 0.0129, Accuracy: 99.54%\n",
      "Train Epoch: [ 11/ 20] Batch number: 1001, Training: Loss: 0.0134, Accuracy: 99.52%\n",
      "Train Epoch: [ 11/ 20] Batch number: 1101, Training: Loss: 0.0136, Accuracy: 99.51%\n",
      "Train Epoch: [ 11/ 20] Batch number: 1201, Training: Loss: 0.0139, Accuracy: 99.50%\n",
      "Train Epoch: [ 11/ 20] Batch number: 1301, Training: Loss: 0.0141, Accuracy: 99.50%\n",
      "Train Epoch: [ 11/ 20] Batch number: 1401, Training: Loss: 0.0140, Accuracy: 99.50%\n",
      "Train Epoch: [ 11/ 20] Batch number: 1501, Training: Loss: 0.0138, Accuracy: 99.51%\n",
      "Train Epoch: [ 11/ 20] Batch number: 1601, Training: Loss: 0.0137, Accuracy: 99.51%\n",
      "Train Epoch: [ 11/ 20] Batch number: 1701, Training: Loss: 0.0136, Accuracy: 99.51%\n",
      "Train Epoch: [ 11/ 20] Batch number: 1801, Training: Loss: 0.0135, Accuracy: 99.52%\n",
      "--- 46.12 minutes ---\n",
      "Training\tEpoch: [11/20]\tLoss: 0.0133\tAccuracy: 0.9952\n",
      "--- 46.12 minutes ---\n",
      "Train Epoch: [ 12/ 20] Batch number:   1, Training: Loss: 0.0143, Accuracy: 99.22%\n",
      "Train Epoch: [ 12/ 20] Batch number: 101, Training: Loss: 0.0132, Accuracy: 99.49%\n",
      "Train Epoch: [ 12/ 20] Batch number: 201, Training: Loss: 0.0122, Accuracy: 99.55%\n",
      "Train Epoch: [ 12/ 20] Batch number: 301, Training: Loss: 0.0141, Accuracy: 99.51%\n",
      "Train Epoch: [ 12/ 20] Batch number: 401, Training: Loss: 0.0141, Accuracy: 99.50%\n",
      "Train Epoch: [ 12/ 20] Batch number: 501, Training: Loss: 0.0150, Accuracy: 99.47%\n",
      "Train Epoch: [ 12/ 20] Batch number: 601, Training: Loss: 0.0146, Accuracy: 99.48%\n",
      "Train Epoch: [ 12/ 20] Batch number: 701, Training: Loss: 0.0143, Accuracy: 99.50%\n",
      "Train Epoch: [ 12/ 20] Batch number: 801, Training: Loss: 0.0142, Accuracy: 99.50%\n",
      "Train Epoch: [ 12/ 20] Batch number: 901, Training: Loss: 0.0146, Accuracy: 99.49%\n",
      "Train Epoch: [ 12/ 20] Batch number: 1001, Training: Loss: 0.0145, Accuracy: 99.50%\n",
      "Train Epoch: [ 12/ 20] Batch number: 1101, Training: Loss: 0.0144, Accuracy: 99.50%\n",
      "Train Epoch: [ 12/ 20] Batch number: 1201, Training: Loss: 0.0147, Accuracy: 99.48%\n",
      "Train Epoch: [ 12/ 20] Batch number: 1301, Training: Loss: 0.0146, Accuracy: 99.49%\n",
      "Train Epoch: [ 12/ 20] Batch number: 1401, Training: Loss: 0.0144, Accuracy: 99.50%\n",
      "Train Epoch: [ 12/ 20] Batch number: 1501, Training: Loss: 0.0142, Accuracy: 99.50%\n",
      "Train Epoch: [ 12/ 20] Batch number: 1601, Training: Loss: 0.0139, Accuracy: 99.51%\n",
      "Train Epoch: [ 12/ 20] Batch number: 1701, Training: Loss: 0.0137, Accuracy: 99.51%\n",
      "Train Epoch: [ 12/ 20] Batch number: 1801, Training: Loss: 0.0137, Accuracy: 99.51%\n",
      "--- 50.42 minutes ---\n",
      "Training\tEpoch: [12/20]\tLoss: 0.0137\tAccuracy: 0.9951\n",
      "--- 50.42 minutes ---\n",
      "Train Epoch: [ 13/ 20] Batch number:   1, Training: Loss: 0.0243, Accuracy: 98.44%\n",
      "Train Epoch: [ 13/ 20] Batch number: 101, Training: Loss: 0.0211, Accuracy: 99.31%\n",
      "Train Epoch: [ 13/ 20] Batch number: 201, Training: Loss: 0.0146, Accuracy: 99.53%\n",
      "Train Epoch: [ 13/ 20] Batch number: 301, Training: Loss: 0.0125, Accuracy: 99.60%\n",
      "Train Epoch: [ 13/ 20] Batch number: 401, Training: Loss: 0.0125, Accuracy: 99.59%\n",
      "Train Epoch: [ 13/ 20] Batch number: 501, Training: Loss: 0.0121, Accuracy: 99.59%\n",
      "Train Epoch: [ 13/ 20] Batch number: 601, Training: Loss: 0.0113, Accuracy: 99.62%\n",
      "Train Epoch: [ 13/ 20] Batch number: 701, Training: Loss: 0.0112, Accuracy: 99.62%\n",
      "Train Epoch: [ 13/ 20] Batch number: 801, Training: Loss: 0.0114, Accuracy: 99.61%\n",
      "Train Epoch: [ 13/ 20] Batch number: 901, Training: Loss: 0.0118, Accuracy: 99.60%\n",
      "Train Epoch: [ 13/ 20] Batch number: 1001, Training: Loss: 0.0118, Accuracy: 99.60%\n",
      "Train Epoch: [ 13/ 20] Batch number: 1101, Training: Loss: 0.0118, Accuracy: 99.60%\n",
      "Train Epoch: [ 13/ 20] Batch number: 1201, Training: Loss: 0.0120, Accuracy: 99.59%\n",
      "Train Epoch: [ 13/ 20] Batch number: 1301, Training: Loss: 0.0121, Accuracy: 99.59%\n",
      "Train Epoch: [ 13/ 20] Batch number: 1401, Training: Loss: 0.0120, Accuracy: 99.59%\n",
      "Train Epoch: [ 13/ 20] Batch number: 1501, Training: Loss: 0.0120, Accuracy: 99.59%\n",
      "Train Epoch: [ 13/ 20] Batch number: 1601, Training: Loss: 0.0122, Accuracy: 99.58%\n",
      "Train Epoch: [ 13/ 20] Batch number: 1701, Training: Loss: 0.0122, Accuracy: 99.58%\n",
      "Train Epoch: [ 13/ 20] Batch number: 1801, Training: Loss: 0.0124, Accuracy: 99.57%\n",
      "--- 54.72 minutes ---\n",
      "Training\tEpoch: [13/20]\tLoss: 0.0124\tAccuracy: 0.9957\n",
      "--- 54.72 minutes ---\n",
      "Train Epoch: [ 14/ 20] Batch number:   1, Training: Loss: 0.0047, Accuracy: 100.00%\n",
      "Train Epoch: [ 14/ 20] Batch number: 101, Training: Loss: 0.0131, Accuracy: 99.52%\n",
      "Train Epoch: [ 14/ 20] Batch number: 201, Training: Loss: 0.0118, Accuracy: 99.61%\n",
      "Train Epoch: [ 14/ 20] Batch number: 301, Training: Loss: 0.0109, Accuracy: 99.64%\n",
      "Train Epoch: [ 14/ 20] Batch number: 401, Training: Loss: 0.0107, Accuracy: 99.66%\n",
      "Train Epoch: [ 14/ 20] Batch number: 501, Training: Loss: 0.0106, Accuracy: 99.66%\n",
      "Train Epoch: [ 14/ 20] Batch number: 601, Training: Loss: 0.0108, Accuracy: 99.65%\n",
      "Train Epoch: [ 14/ 20] Batch number: 701, Training: Loss: 0.0109, Accuracy: 99.64%\n",
      "Train Epoch: [ 14/ 20] Batch number: 801, Training: Loss: 0.0110, Accuracy: 99.63%\n",
      "Train Epoch: [ 14/ 20] Batch number: 901, Training: Loss: 0.0112, Accuracy: 99.62%\n",
      "Train Epoch: [ 14/ 20] Batch number: 1001, Training: Loss: 0.0114, Accuracy: 99.61%\n",
      "Train Epoch: [ 14/ 20] Batch number: 1101, Training: Loss: 0.0117, Accuracy: 99.60%\n",
      "Train Epoch: [ 14/ 20] Batch number: 1201, Training: Loss: 0.0120, Accuracy: 99.59%\n",
      "Train Epoch: [ 14/ 20] Batch number: 1301, Training: Loss: 0.0118, Accuracy: 99.60%\n",
      "Train Epoch: [ 14/ 20] Batch number: 1401, Training: Loss: 0.0117, Accuracy: 99.60%\n"
     ]
    }
   ],
   "source": [
    "# measure data loading time\n",
    "start_time = time.time()\n",
    "\n",
    "#loop throuh epochs\n",
    "for epoch in range(nepochs):\n",
    "    train_dset.shuffletraindata()\n",
    "    train_dset.setmode(2)\n",
    "    loss, acc = train(epoch, train_loader, model, criterion, optimizer)\n",
    "    \n",
    "    # measure elapsed time  so far  \n",
    "    print(\"--- {:0.2f} minutes ---\".format((time.time() - start_time)/60.))\n",
    "    \n",
    "    print('Training\\tEpoch: [{}/{}]\\tLoss: {:0.4f}\\tAccuracy: {:0.4f}'.\n",
    "          format(epoch+1, nepochs, loss, acc))\n",
    "    \n",
    "    fconv = open(os.path.join(output, 'train_convergence.csv'), 'a')\n",
    "    fconv.write('{},{:0.4f},{:0.4f}\\n'.format(epoch+1,loss, acc))\n",
    "    fconv.close()\n",
    "\n",
    "    #Validation if needed --- \n",
    "    if val_lib and (epoch+1) % test_every == 0:\n",
    "        val_dset.setmode(1)\n",
    "        val_probs, val_acc, val_preds = inference(epoch, val_loader, model)\n",
    "        \n",
    "        #aggregating tile scores into slide score - 3 different methods (max, average, and majority voting)\n",
    "        aggregate_slide_predavg = group_avg(np.array(val_dset.slideIDX), val_preds)\n",
    "        aggregate_slide_probavg = group_avg(np.array(val_dset.slideIDX), val_probs)\n",
    "        aggregate_slide_max = group_max(np.array(val_dset.slideIDX), val_probs, len(val_dset.slides))\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(val_dset.targets, aggregate_slide_predavg)\n",
    "        roc_auc_maj_vote = auc(fpr, tpr)\n",
    "        fpr, tpr, thresholds = roc_curve(val_dset.targets, aggregate_slide_probavg)\n",
    "        roc_auc_avg_prob = auc(fpr, tpr)\n",
    "        fpr, tpr, thresholds = roc_curve(val_dset.targets, aggregate_slide_max)\n",
    "        roc_auc_max_prob = auc(fpr, tpr)\n",
    "        \n",
    "        print('Validation\\tEpoch: [{}/{}]\\t val_acc: {:0.4f}\\tROC-AUC: max_prob: {:0.4f}\\t avg_prob: {:0.4f}\\t maj_vote: {:0.4f}\\t best so far: {:0.4f}'\n",
    "              .format(epoch+1, nepochs, val_acc, roc_auc_max_prob, roc_auc_avg_prob, roc_auc_maj_vote, \n",
    "                      max(best_auc_v, roc_auc_max_prob, roc_auc_avg_prob, roc_auc_maj_vote)))\n",
    "        \n",
    "        fconv = open(os.path.join(output, 'valid_convergence.csv'), 'a')\n",
    "        fconv.write('{},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f}\\n'\n",
    "                    .format(epoch+1, val_acc,roc_auc_max_prob, roc_auc_avg_prob, roc_auc_maj_vote, \n",
    "                            max(best_auc_v, roc_auc_max_prob, roc_auc_avg_prob, roc_auc_maj_vote)))\n",
    "        \n",
    "        #Save best model\n",
    "        if max(roc_auc_max_prob, roc_auc_avg_prob, roc_auc_maj_vote) > best_auc_v:\n",
    "            best_auc_v = max(roc_auc_max_prob, roc_auc_avg_prob, roc_auc_maj_vote)\n",
    "            obj = {\n",
    "                'epoch': epoch+1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_auc_v': best_auc_v,\n",
    "                'optimizer' : optimizer.state_dict()\n",
    "            }\n",
    "            torch.save(obj, os.path.join(output,'checkpoint_best.pth'))\n",
    "            \n",
    "    # measure accumulated elapsed time so far \n",
    "    print(\"--- {:0.2f} minutes ---\".format((time.time() - start_time)/60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
